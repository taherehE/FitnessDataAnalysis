{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "\n",
    "import numpy as np\n",
    "import pyreadstat\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from seaborn import scatterplot as scatter\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#import sklearn.metrics.scorer as scorer\n",
    "\n",
    "\n",
    "#reading data from file\n",
    "\n",
    "#df,meta=pyreadstat.read_sav(\"C:/Users/APPLE/Desktop/Folders/sana thesis/SHAHNAZ-meysam-tekvando.sav\")\n",
    "df=pd.read_excel(\"C:/Users/APPLE/Desktop/Folders/sana thesis/SHAHNAZ-meysam-tekvandolabelmodified.xls\")\n",
    "\n",
    "#preprocessing operations such as converting Nan values to zero,\n",
    "#converting fields without value to zero,\n",
    "#removing ID,fisrtname, and familyname columns from dataset\n",
    "#data=np.nan_to_num(data,0)\n",
    "\n",
    "features_name=df.columns\n",
    "data=np.array(df)\n",
    "df=pd.DataFrame(data=data,columns=features_name)\n",
    "df=df.drop(['firstname','familyname','year'],axis=1)\n",
    "\n",
    "# keep only women samples\n",
    "df1=df\n",
    "for l in range(2559):\n",
    "    if (df1.iloc[l,0]==2.0):\n",
    "        df=df.drop([l])    \n",
    "\n",
    "true_labels=df.iloc[:,-1]\n",
    "df=df.drop(['label','gender'],axis=1)\n",
    "features_name=df.columns\n",
    "        \n",
    "true_labels=true_labels.fillna(0)\n",
    "for i in range(len(true_labels)):\n",
    "    if (true_labels.iloc[i]==0):\n",
    "        true_labels.iloc[i]=-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(['HR','Concani','burus','ArmSpan','Vo2Max','WingtePP','wingetAP','ricaveri','oneMayl','pareshTool','joftDoTaraf','barfix','marpych','jaheshJanebi',\n",
    "            'FlexibleTanehBePosht','FlexibleShoulder','FlexibleRan','RTshenidari','tawanBiHavazi','biHAVA1609','Do1600','yekMayl',\n",
    "            'metr540','taadol'],axis=1)\n",
    "features_name=df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\APPLE\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\impute\\_iterative.py:638: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  \" reached.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import MissingIndicator\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "data=np.array(df)\n",
    "data=data.astype('float')\n",
    "data[data==0.0]=np.nan\n",
    "df=pd.DataFrame(data=data,columns=features_name)\n",
    "\n",
    "strategy=('mean','median')\n",
    "simpleimputer=SimpleImputer(missing_values=np.nan, strategy=strategy[1])\n",
    "data1=simpleimputer.fit_transform(df)\n",
    "#df1=pd.DataFrame(data=data1,columns=features_name)\n",
    "#simpleimputer=SimpleImputer(missing_values=0.0, strategy=strategy[0])\n",
    "#data1=simpleimputer.fit_transform(df1)\n",
    "\n",
    "\n",
    "knnimputer=KNNImputer(missing_values=np.nan)\n",
    "data2=knnimputer.fit_transform(df)\n",
    "#df2=pd.DataFrame(data=data2,columns=features_name)\n",
    "#knnimputer=KNNImputer(missing_values=0.0)\n",
    "#data2=knnimputer.fit_transform(df2)\n",
    "\n",
    "estimators=[BayesianRidge(),\n",
    "            DecisionTreeRegressor(max_features='sqrt',random_state=0),\n",
    "            ExtraTreesRegressor(n_estimators=10, random_state=0),\n",
    "            KNeighborsRegressor(n_neighbors=15)\n",
    "           ]\n",
    "\n",
    "iterativeimputer=IterativeImputer(missing_values=np.nan, random_state=0, estimator=estimators[1], n_nearest_features=5)\n",
    "data3=iterativeimputer.fit_transform(df)\n",
    "#df3=pd.DataFrame(data=data3,columns=features_name)\n",
    "#iterativeimputer=IterativeImputer(missing_values=0.0, random_state=0, estimator=estimators[0], n_nearest_features=5)\n",
    "#data3=iterativeimputer.fit_transform(df3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing standardScaler as preprocessing\n",
    "\n",
    "scaler=StandardScaler(with_mean=False)\n",
    "data_scaled=scaler.fit_transform(data2)\n",
    "df=pd.DataFrame(data=data_scaled,columns=features_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep 70% of labeled data for test\n",
    "true_labels=true_labels.astype(float)\n",
    "true_labels.reset_index(inplace=True, drop=True)\n",
    "\n",
    "df['label']=true_labels\n",
    "df3=df\n",
    "df4=df\n",
    "\n",
    "# making df3 as a labeled set\n",
    "for l in range(len(df)):\n",
    "    if(df.iloc[l,-1]==-1):\n",
    "        df3=df3.drop([l])\n",
    "        \n",
    "df3_labels=df3.iloc[:,-1]\n",
    "df5=df3\n",
    "#df3=df3.drop(['label'],axis=1)\n",
    "X_train_labeled,X_test=train_test_split(df3,test_size=0.60,random_state=42)\n",
    "\n",
    "#making df4 as an unlabeled set\n",
    "for l in range(len(df)):\n",
    "    for i in range(len(df5)):\n",
    "        if (df.iloc[l,:].equals(df5.iloc[i,:])):\n",
    "            df4=df4.drop([l])\n",
    "            break\n",
    "\n",
    "X_train=df4\n",
    "X_train=X_train.append(X_train_labeled)\n",
    "\n",
    "Y_train_labeled=X_train_labeled.iloc[:,-1]\n",
    "X_train_labeled=X_train_labeled.drop(['label'],axis=1)\n",
    "Y_train_labeled=Y_train_labeled.to_frame()\n",
    "Y_train_labeled=Y_train_labeled.astype(float)\n",
    "Y_train_labeled=Y_train_labeled.reset_index(drop=True)\n",
    "X_train_labeled=X_train_labeled.astype(float)\n",
    "X_train_labeled=X_train_labeled.reset_index( drop=True)\n",
    "\n",
    "Y_train=X_train.iloc[:,-1]\n",
    "Y_test=X_test.iloc[:,-1]\n",
    "X_train=X_train.drop(['label'],axis=1)\n",
    "X_test=X_test.drop(['label'],axis=1)\n",
    "Y_train=Y_train.astype(float)\n",
    "Y_train=Y_train.reset_index(drop=True)\n",
    "Y_test=Y_test.astype(float)\n",
    "Y_test=Y_test.reset_index( drop=True)\n",
    "X_train=X_train.astype(float)\n",
    "X_test=X_test.astype(float)\n",
    "X_train=X_train.reset_index( drop=True)\n",
    "X_test=X_test.reset_index( drop=True)\n",
    "Y_train=Y_train.to_frame()\n",
    "Y_test=Y_test.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess for class 1\n",
    "\n",
    "Y_train=Y_train.replace(-1,-2)\n",
    "true_labels=true_labels.replace(-1,-2)\n",
    "\n",
    "Y_train=Y_train.replace(2,-1)\n",
    "Y_train=Y_train.replace(3,-1)\n",
    "\n",
    "true_labels=true_labels.replace(-1,-2)\n",
    "true_labels=true_labels.replace(2,-1)\n",
    "true_labels=true_labels.replace(3,-1)\n",
    "\n",
    "Y_test=Y_test.replace(-1,-2)\n",
    "\n",
    "Y_test=Y_test.replace(2,-1)\n",
    "Y_test=Y_test.replace(3,-1)\n",
    "\n",
    "Y_train_labeled=Y_train_labeled.replace(2,-1)\n",
    "Y_train_labeled=Y_train_labeled.replace(3,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess for class 2 \n",
    "\n",
    "Y_train=Y_train.replace(-1,-2)\n",
    "\n",
    "Y_train=Y_train.replace(1,-1)\n",
    "Y_train=Y_train.replace(3,-1)\n",
    "Y_train=Y_train.replace(2,1)\n",
    "\n",
    "true_labels=true_labels.replace(-1,-2)\n",
    "\n",
    "true_labels=true_labels.replace(1,-1)\n",
    "true_labels=true_labels.replace(3,-1)\n",
    "true_labels=true_labels.replace(2,1)\n",
    "\n",
    "Y_test=Y_test.replace(-1,-2)\n",
    "\n",
    "Y_test=Y_test.replace(1,-1)\n",
    "Y_test=Y_test.replace(3,-1)\n",
    "Y_test=Y_test.replace(2,1)\n",
    "\n",
    "\n",
    "Y_train_labeled=Y_train_labeled.replace(1,-1)\n",
    "Y_train_labeled=Y_train_labeled.replace(3,-1)\n",
    "Y_train_labeled=Y_train_labeled.replace(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess for class 3 \n",
    "\n",
    "Y_train=Y_train.replace(-1,-2)\n",
    "\n",
    "Y_train=Y_train.replace(1,-1)\n",
    "Y_train=Y_train.replace(2,-1)\n",
    "Y_train=Y_train.replace(3,1)\n",
    "\n",
    "true_labels=true_labels.replace(-1,-2)\n",
    "\n",
    "true_labels=true_labels.replace(1,-1)\n",
    "true_labels=true_labels.replace(2,-1)\n",
    "true_labels=true_labels.replace(3,1)\n",
    "\n",
    "Y_test=Y_test.replace(-1,-2)\n",
    "\n",
    "Y_test=Y_test.replace(1,-1)\n",
    "Y_test=Y_test.replace(2,-1)\n",
    "Y_test=Y_test.replace(3,1)\n",
    "\n",
    "\n",
    "Y_train_labeled=Y_train_labeled.replace(1,-1)\n",
    "Y_train_labeled=Y_train_labeled.replace(2,-1)\n",
    "Y_train_labeled=Y_train_labeled.replace(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report of SemiBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.62      0.96      0.76        26\n",
      "         1.0       0.75      0.17      0.27        18\n",
      "\n",
      "    accuracy                           0.64        44\n",
      "   macro avg       0.69      0.56      0.52        44\n",
      "weighted avg       0.68      0.64      0.56        44\n",
      "\n",
      "confusion_matrix\n",
      "[[25  1]\n",
      " [15  3]]\n",
      "classification report of Base Classifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.57      0.92      0.71        26\n",
      "         1.0       0.00      0.00      0.00        18\n",
      "\n",
      "    accuracy                           0.55        44\n",
      "   macro avg       0.29      0.46      0.35        44\n",
      "weighted avg       0.34      0.55      0.42        44\n",
      "\n",
      "confusion_matrix\n",
      "[[24  2]\n",
      " [18  0]]\n"
     ]
    }
   ],
   "source": [
    "# SemiBoost\n",
    "import sys\n",
    "sys.path.append('C:/Users/APPLE/Desktop/Folders/sana thesis/semi-supervised/semi_boost-master - Copy - Copy/semi_boost-master/src')\n",
    "import SemiBoost\n",
    "import utils\n",
    "from sklearn.datasets import make_classification, make_gaussian_quantiles, make_blobs, make_moons, make_circles\n",
    "\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.decomposition import DictionaryLearning\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.spatial.distance import pdist,squareform\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import importlib\n",
    "\n",
    "ROC_semiboost = list()\n",
    "ROC_clf = list()\n",
    "\n",
    "X_train=np.array(X_train)\n",
    "Y_train=np.array(Y_train)\n",
    "Y_train=np.squeeze(Y_train)\n",
    "\n",
    "X_test=np.array(X_test)\n",
    "Y_test=np.array(Y_test)\n",
    "Y2_test=Y_test\n",
    "Y_test=np.squeeze(Y_test)\n",
    "\n",
    "X_train_labeled=np.array(X_train_labeled)\n",
    "Y_train_labeled=np.array(Y_train_labeled)\n",
    "Y_train_labeled=np.squeeze(Y_train_labeled)\n",
    "\n",
    "\n",
    "#feature extraction algorithms\n",
    "\n",
    "#svd = TruncatedSVD(n_components=2, algorithm='randomized')\n",
    "#data_extracted=svd.fit_transform(X_train)\n",
    "#data_test_extracted=svd.fit_transform(X_test)\n",
    "#data_train_labeled_extracted=svd.fit_transform(X_train_labeled)\n",
    "\n",
    "nmf=NMF(n_components=2, init='random', random_state=0)\n",
    "data_extracted=nmf.fit_transform(X_train)\n",
    "data_test_extracted=nmf.fit_transform(X_test)\n",
    "data_train_labeled_extracted=nmf.fit_transform(X_train_labeled)\n",
    "\n",
    "#fastICA=FastICA(n_components=2, random_state=0)\n",
    "#data_extracted=fastICA.fit_transform(X_train)\n",
    "#data_test_extracted=fastICA.fit_transform(X_test)\n",
    "#data_train_labeled_extracted=fastICA.fit_transform(X_train_labeled)\n",
    "\n",
    "#kpca=KernelPCA(n_components=2,kernel='poly')\n",
    "#data_extracted=kpca.fit_transform(X_train)\n",
    "#data_test_extracted=kpca.fit_transform(X_test)\n",
    "#data_train_labeled_extracted=kpca.fit_transform(X_train_labeled)\n",
    "\n",
    "#lda=LatentDirichletAllocation(n_components=2, random_state=0)\n",
    "#data_extracted=lda.fit_transform(X_train)\n",
    "#data_test_extracted=lda.fit_transform(X_test)\n",
    "#data_train_labeled_extracted=lda.fit_transform(X_train_labeled)\n",
    "\n",
    "#pca=PCA(n_components=2)\n",
    "#data_extracted=pca.fit_transform(X_train)\n",
    "#data_test_extracted=pca.fit_transform(X_test)\n",
    "#data_train_labeled_extracted=pca.fit_transform(X_train_labeled)\n",
    "\n",
    "#fa=FactorAnalysis(n_components=2, random_state=0)\n",
    "#data_extracted=fa.fit_transform(X_train)\n",
    "#data_test_extracted=fa.fit_transform(X_test)\n",
    "#data_train_labeled_extracted=fa.fit_transform(X_train_labeled)\n",
    "\n",
    "#dl=DictionaryLearning(n_components=2, random_state=0, fit_algorithm='cd',transform_algorithm='lasso_cd')\n",
    "#data_extracted=fa.fit_transform(X_train)\n",
    "#data_test_extracted=fa.fit_transform(X_test)\n",
    "#data_train_labeled_extracted=fa.fit_transform(X_train_labeled)\n",
    "\n",
    "''' SEMIBOOST SKLEARN STYLE '''\n",
    "\n",
    "model=SemiBoost.SemiBoostClassifier(base_model =SVC(probability = True))\n",
    "model.fit(data_extracted,Y_train,n_neighbors = 3, n_jobs = 10, max_models = 15, similarity_kernel='rbf', verbose = False)\n",
    "y_predicted=model.predict(data_test_extracted)\n",
    "ROC_semiboost.append(roc_auc_score(Y_test,model.predict(data_test_extracted)))\n",
    "cm=confusion_matrix(Y2_test, y_predicted)\n",
    "\n",
    "print(\"classification report of SemiBoost\")\n",
    "print(classification_report(Y2_test, y_predicted))\n",
    "print(\"confusion_matrix\")\n",
    "print(cm)\n",
    "\n",
    "''' BASE CLASSIFIER '''\n",
    "\n",
    "clf= SVC(probability = True)\n",
    "clf.fit(data_train_labeled_extracted,Y_train_labeled)\n",
    "y_predicted=clf.predict(data_test_extracted)\n",
    "ROC_clf.append(roc_auc_score(Y_test,model.predict(data_test_extracted)))\n",
    "\n",
    "\n",
    "#print(\"the mean of ROC_semiboost-ROC_BaseClassifier \"+str(np.mean(np.array(ROC_semiboost)-np.array(ROC_clf))))\n",
    "#print(\"the standard deviation of ROC_semiboost-ROC_BaseClassifier \"+ str(np.std(np.array(ROC_semiboost)-np.array(ROC_clf))))\n",
    "\n",
    "cm=confusion_matrix(Y2_test, y_predicted)\n",
    "\n",
    "print(\"classification report of Base Classifier\")\n",
    "print(classification_report(Y2_test, y_predicted))\n",
    "print(\"confusion_matrix\")\n",
    "print(cm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Plot Test data'''\n",
    "gs = gridspec.GridSpec(1, 2)\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "y=Y2_test\n",
    "y=y.astype(np.integer)\n",
    "y=np.squeeze(y)\n",
    "\n",
    "ax = plt.subplot(gs[0, 0])\n",
    "fig = plot_decision_regions(X=data_test_extracted, y=y, clf=model, legend=2)\n",
    "plt.title('SemiBoost')\n",
    "\n",
    "\n",
    "''' Plot '''\n",
    "ax = plt.subplot(gs[0, 1])\n",
    "fig = plot_decision_regions(X=data_test_extracted, y=y, clf=clf, legend=2)\n",
    "plt.title('BaseModel')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemiBoost for whole data\n",
    "import sys\n",
    "sys.path.append('C:/Users/APPLE/Desktop/Folders/sana thesis/semi-supervised/semi_boost-master - Copy - Copy/semi_boost-master/src')\n",
    "import SemiBoost\n",
    "import utils\n",
    "from sklearn.datasets import make_classification, make_gaussian_quantiles, make_blobs, make_moons, make_circles\n",
    "\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.decomposition import DictionaryLearning\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.spatial.distance import pdist,squareform\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import importlib\n",
    "\n",
    "ROC_semiboost = list()\n",
    "ROC_clf = list()\n",
    "\n",
    "\n",
    "whole_data=df.drop(['label'],axis=1)\n",
    "whole_data=np.array(whole_data)\n",
    "true_labels=np.array(true_labels)\n",
    "true_labels=np.squeeze(true_labels)\n",
    "\n",
    "\n",
    "#feature extraction algorithms\n",
    "\n",
    "#svd = TruncatedSVD(n_components=2, algorithm='randomized')\n",
    "#data_extracted=svd.fit_transform(X_train)\n",
    "#data_test_extracted=svd.fit_transform(X_test)\n",
    "#data_train_labeled_extracted=svd.fit_transform(X_train_labeled)\n",
    "\n",
    "nmf=NMF(n_components=2, init='random', random_state=0)\n",
    "whole_data_extracted=nmf.fit_transform(whole_data)\n",
    "\n",
    "\n",
    "#fastICA=FastICA(n_components=2, random_state=0)\n",
    "#data_extracted=fastICA.fit_transform(X_train)\n",
    "#data_test_extracted=fastICA.fit_transform(X_test)\n",
    "#data_train_labeled_extracted=fastICA.fit_transform(X_train_labeled)\n",
    "\n",
    "#kpca=KernelPCA(n_components=2,kernel='poly')\n",
    "#data_extracted=kpca.fit_transform(X_train)\n",
    "#data_test_extracted=kpca.fit_transform(X_test)\n",
    "#data_train_labeled_extracted=kpca.fit_transform(X_train_labeled)\n",
    "\n",
    "#lda=LatentDirichletAllocation(n_components=2, random_state=0)\n",
    "#data_extracted=lda.fit_transform(X_train)\n",
    "#data_test_extracted=lda.fit_transform(X_test)\n",
    "#data_train_labeled_extracted=lda.fit_transform(X_train_labeled)\n",
    "\n",
    "#pca=PCA(n_components=2)\n",
    "#data_extracted=pca.fit_transform(X_train)\n",
    "#data_test_extracted=pca.fit_transform(X_test)\n",
    "#data_train_labeled_extracted=pca.fit_transform(X_train_labeled)\n",
    "\n",
    "#fa=FactorAnalysis(n_components=2, random_state=0)\n",
    "#data_extracted=fa.fit_transform(X_train)\n",
    "#data_test_extracted=fa.fit_transform(X_test)\n",
    "#data_train_labeled_extracted=fa.fit_transform(X_train_labeled)\n",
    "\n",
    "#dl=DictionaryLearning(n_components=2, random_state=0, fit_algorithm='cd',transform_algorithm='lasso_cd')\n",
    "#data_extracted=fa.fit_transform(X_train)\n",
    "#data_test_extracted=fa.fit_transform(X_test)\n",
    "#data_train_labeled_extracted=fa.fit_transform(X_train_labeled)\n",
    "\n",
    "''' SEMIBOOST SKLEARN STYLE '''\n",
    "\n",
    "model=SemiBoost.SemiBoostClassifier(base_model =SVC(probability = True))\n",
    "model.fit(whole_data_extracted,true_labels,n_neighbors = 3, n_jobs = 10, max_models = 15, similarity_kernel='rbf', verbose = False)\n",
    "y_predicted=model.predict(whole_data_extracted)\n",
    "\n",
    "\n",
    "#print(\"the mean of ROC_semiboost-ROC_BaseClassifier \"+str(np.mean(np.array(ROC_semiboost)-np.array(ROC_clf))))\n",
    "#print(\"the standard deviation of ROC_semiboost-ROC_BaseClassifier \"+ str(np.std(np.array(ROC_semiboost)-np.array(ROC_clf))))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Plot whole data'''\n",
    "gs = gridspec.GridSpec(1, 2)\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "y=y_predicted\n",
    "y=y.astype(np.integer)\n",
    "y=np.squeeze(y)\n",
    "\n",
    "y_true=true_labels\n",
    "y_true=y_true.astype(np.integer)\n",
    "y_true=np.squeeze(y_true)\n",
    "\n",
    "ax = plt.subplot(gs[0, 0])\n",
    "fig = plot_decision_regions(X=whole_data_extracted, y=y, clf=model, legend=2)\n",
    "plt.title('SemiBoost')\n",
    "fig=plt.xlabel(\"NMF First Principle Component\")\n",
    "fig=plt.ylabel(\"NMF second Principle Component\")\n",
    "plt.legend(scatterpoints=1, shadow=False, loc='upper right')\n",
    "\n",
    "\n",
    "\n",
    "ax = plt.subplot(gs[0, 1])\n",
    "fig=plt.scatter(whole_data_extracted[true_labels==-2, 0],whole_data_extracted[true_labels==-2, 1],color='c',marker='o', lw=0, label=\"unknown\", s=30)\n",
    "fig=plt.scatter(whole_data_extracted[true_labels==1, 0],whole_data_extracted[true_labels==1, 1],color='darkorange',marker='^', lw=0, label=\"Bronze medal\", s=30)\n",
    "fig=plt.scatter(whole_data_extracted[true_labels==-1, 0],whole_data_extracted[true_labels==-1, 1],color='navy',marker='s', lw=0, label=\"silver and gold medal\", s=30)\n",
    "fig=plt.xlabel(\"NMF First Principle Component\")\n",
    "fig=plt.ylabel(\"NMF second Principle Component\")\n",
    "plt.legend(scatterpoints=1, shadow=False, loc='upper right')\n",
    "plt.title('Raw_whole_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/tmadl/semisup-learn.git#egg=semisup-learn\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
